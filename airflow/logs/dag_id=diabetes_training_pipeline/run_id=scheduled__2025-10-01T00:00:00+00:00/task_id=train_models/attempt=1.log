[2025-10-29T01:10:44.069+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: diabetes_training_pipeline.train_models scheduled__2025-10-01T00:00:00+00:00 [queued]>
[2025-10-29T01:10:44.163+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: diabetes_training_pipeline.train_models scheduled__2025-10-01T00:00:00+00:00 [queued]>
[2025-10-29T01:10:44.163+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 2
[2025-10-29T01:10:44.176+0000] {taskinstance.py:1382} INFO - Executing <Task(PythonOperator): train_models> on 2025-10-01 00:00:00+00:00
[2025-10-29T01:10:44.190+0000] {standard_task_runner.py:57} INFO - Started process 1950 to run task
[2025-10-29T01:10:44.196+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'diabetes_training_pipeline', 'train_models', 'scheduled__2025-10-01T00:00:00+00:00', '--job-id', '14', '--raw', '--subdir', 'DAGS_FOLDER/diabetes_training_pipeline.py', '--cfg-path', '/tmp/tmpf2c6fsc7']
[2025-10-29T01:10:44.202+0000] {standard_task_runner.py:85} INFO - Job 14: Subtask train_models
[2025-10-29T01:10:44.274+0000] {task_command.py:416} INFO - Running <TaskInstance: diabetes_training_pipeline.train_models scheduled__2025-10-01T00:00:00+00:00 [running]> on host 8f23d0ceb5b8
[2025-10-29T01:10:44.369+0000] {taskinstance.py:1662} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='mlops_team' AIRFLOW_CTX_DAG_ID='diabetes_training_pipeline' AIRFLOW_CTX_TASK_ID='train_models' AIRFLOW_CTX_EXECUTION_DATE='2025-10-01T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-10-01T00:00:00+00:00'
[2025-10-29T01:10:46.721+0000] {logging_mixin.py:154} WARNING - 2025/10/29 01:10:46 INFO mlflow.tracking.fluent: Experiment with name 'diabetes_readmission_stage2' does not exist. Creating a new experiment.
[2025-10-29T01:10:47.098+0000] {taskinstance.py:1937} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py", line 192, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py", line 209, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/diabetes_training_pipeline.py", line 103, in train_models
    mlflow.set_experiment(EXPERIMENT_NAME)
  File "/home/airflow/.local/lib/python3.10/site-packages/mlflow/tracking/fluent.py", line 144, in set_experiment
    experiment_id = client.create_experiment(experiment_name)
  File "/home/airflow/.local/lib/python3.10/site-packages/mlflow/tracking/client.py", line 570, in create_experiment
    return self._tracking_client.create_experiment(name, artifact_location, tags)
  File "/home/airflow/.local/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 235, in create_experiment
    return self.store.create_experiment(
  File "/home/airflow/.local/lib/python3.10/site-packages/mlflow/store/tracking/rest_store.py", line 98, in create_experiment
    response_proto = self._call_endpoint(CreateExperiment, req_body)
  File "/home/airflow/.local/lib/python3.10/site-packages/mlflow/store/tracking/rest_store.py", line 59, in _call_endpoint
    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)
  File "/home/airflow/.local/lib/python3.10/site-packages/mlflow/utils/rest_utils.py", line 219, in call_endpoint
    response = verify_rest_response(response, endpoint)
  File "/home/airflow/.local/lib/python3.10/site-packages/mlflow/utils/rest_utils.py", line 151, in verify_rest_response
    raise RestException(json.loads(response.text))
mlflow.exceptions.RestException: RESOURCE_ALREADY_EXISTS: Experiment(name=diabetes_readmission_stage2) already exists. Error: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(pymysql.err.IntegrityError) (1062, "Duplicate entry 'diabetes_readmission_stage2' for key 'experiments.name'")
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (%(name)s, %(artifact_location)s, %(lifecycle_stage)s, %(creation_time)s, %(last_update_time)s)]
[parameters: {'name': 'diabetes_readmission_stage2', 'artifact_location': '', 'lifecycle_stage': 'active', 'creation_time': 1761700246837, 'last_update_time': 1761700246837}]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
[2025-10-29T01:10:47.129+0000] {taskinstance.py:1400} INFO - Marking task as UP_FOR_RETRY. dag_id=diabetes_training_pipeline, task_id=train_models, execution_date=20251001T000000, start_date=20251029T011044, end_date=20251029T011047
[2025-10-29T01:10:47.153+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 14 for task train_models (RESOURCE_ALREADY_EXISTS: Experiment(name=diabetes_readmission_stage2) already exists. Error: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(pymysql.err.IntegrityError) (1062, "Duplicate entry 'diabetes_readmission_stage2' for key 'experiments.name'")
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (%(name)s, %(artifact_location)s, %(lifecycle_stage)s, %(creation_time)s, %(last_update_time)s)]
[parameters: {'name': 'diabetes_readmission_stage2', 'artifact_location': '', 'lifecycle_stage': 'active', 'creation_time': 1761700246837, 'last_update_time': 1761700246837}]
(Background on this error at: https://sqlalche.me/e/20/gkpj); 1950)
[2025-10-29T01:10:47.206+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2025-10-29T01:10:47.263+0000] {taskinstance.py:2778} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-11-01T22:46:42.868+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: diabetes_training_pipeline.train_models scheduled__2025-10-01T00:00:00+00:00 [queued]>
[2025-11-01T22:46:43.141+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: diabetes_training_pipeline.train_models scheduled__2025-10-01T00:00:00+00:00 [queued]>
[2025-11-01T22:46:43.142+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 2
[2025-11-01T22:46:43.150+0000] {taskinstance.py:1382} INFO - Executing <Task(PythonOperator): train_models> on 2025-10-01 00:00:00+00:00
[2025-11-01T22:46:43.167+0000] {standard_task_runner.py:57} INFO - Started process 4263 to run task
[2025-11-01T22:46:43.174+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'diabetes_training_pipeline', 'train_models', 'scheduled__2025-10-01T00:00:00+00:00', '--job-id', '6', '--raw', '--subdir', 'DAGS_FOLDER/diabetes_training_pipeline.py', '--cfg-path', '/tmp/tmp03ft3r8p']
[2025-11-01T22:46:43.179+0000] {standard_task_runner.py:85} INFO - Job 6: Subtask train_models
[2025-11-01T22:46:43.226+0000] {task_command.py:416} INFO - Running <TaskInstance: diabetes_training_pipeline.train_models scheduled__2025-10-01T00:00:00+00:00 [running]> on host abe2ec3976bd
[2025-11-01T22:46:43.268+0000] {taskinstance.py:1662} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='mlops_team' AIRFLOW_CTX_DAG_ID='diabetes_training_pipeline' AIRFLOW_CTX_TASK_ID='train_models' AIRFLOW_CTX_EXECUTION_DATE='2025-10-01T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-10-01T00:00:00+00:00'
[2025-11-01T22:46:44.625+0000] {logging_mixin.py:154} WARNING - 2025/11/01 22:46:44 INFO mlflow.tracking.fluent: Experiment with name 'diabetes_readmission_stage2' does not exist. Creating a new experiment.
[2025-11-01T22:46:44.714+0000] {logging_mixin.py:154} WARNING - 2025/11/01 22:46:44 WARNING mlflow.utils.git_utils: Failed to import Git (the Git executable is probably not on your PATH), so Git SHA is not available. Error: Failed to initialize: Bad git executable.
The git executable must be specified in one of the following ways:
    - be included in your $PATH
    - be set via $GIT_PYTHON_GIT_EXECUTABLE
    - explicitly set via git.refresh(<full-path-to-git-executable>)

All git commands will error until this is rectified.

This initial message can be silenced or aggravated in the future by setting the
$GIT_PYTHON_REFRESH environment variable. Use one of the following values:
    - quiet|q|silence|s|silent|none|n|0: for no message or exception
    - warn|w|warning|log|l|1: for a warning message (logging level CRITICAL, displayed by default)
    - error|e|exception|raise|r|2: for a raised exception

Example:
    export GIT_PYTHON_REFRESH=quiet
[2025-11-01T22:46:46.802+0000] {logging_mixin.py:154} WARNING - 2025/11/01 22:46:46 WARNING mlflow.models.signature: Failed to infer the model signature from the input example. Reason: MlflowException("Unable to map 'object' type to MLflow DataType. object can be mapped iff all values have identical data type which is one of (string, (bytes or byterray),  int, float)."). To see the full traceback, set the logging level to DEBUG via `logging.getLogger("mlflow").setLevel(logging.DEBUG)`. To disable automatic signature inference, set `signature` to `False` in your `log_model` or `save_model` call.
[2025-11-01T22:46:49.785+0000] {logging_mixin.py:154} WARNING - /home/***/.local/lib/python3.10/site-packages/_distutils_hack/__init__.py:33 UserWarning: Setuptools is replacing distutils.
[2025-11-01T22:46:49.961+0000] {credentials.py:1147} INFO - Found credentials in environment variables.
[2025-11-01T22:46:51.039+0000] {taskinstance.py:1937} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/boto3/s3/transfer.py", line 371, in upload_file
    future.result()
  File "/home/airflow/.local/lib/python3.10/site-packages/s3transfer/futures.py", line 103, in result
    return self._coordinator.result()
  File "/home/airflow/.local/lib/python3.10/site-packages/s3transfer/futures.py", line 264, in result
    raise self._exception
  File "/home/airflow/.local/lib/python3.10/site-packages/s3transfer/tasks.py", line 135, in __call__
    return self._execute_main(kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/s3transfer/tasks.py", line 158, in _execute_main
    return_value = self._main(**kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/s3transfer/upload.py", line 762, in _main
    client.put_object(Bucket=bucket, Key=key, Body=body, **extra_args)
  File "/home/airflow/.local/lib/python3.10/site-packages/botocore/client.py", line 565, in _api_call
    return self._make_api_call(operation_name, kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/botocore/client.py", line 1017, in _make_api_call
    raise error_class(parsed_response, operation_name)
botocore.errorfactory.NoSuchBucket: An error occurred (NoSuchBucket) when calling the PutObject operation: The specified bucket does not exist

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py", line 192, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py", line 209, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/diabetes_training_pipeline.py", line 158, in train_models
    mlflow.sklearn.log_model(
  File "/home/airflow/.local/lib/python3.10/site-packages/mlflow/sklearn/__init__.py", line 411, in log_model
    return Model.log(
  File "/home/airflow/.local/lib/python3.10/site-packages/mlflow/models/model.py", line 620, in log
    mlflow.tracking.fluent.log_artifacts(local_path, mlflow_model.artifact_path)
  File "/home/airflow/.local/lib/python3.10/site-packages/mlflow/tracking/fluent.py", line 1030, in log_artifacts
    MlflowClient().log_artifacts(run_id, local_dir, artifact_path)
  File "/home/airflow/.local/lib/python3.10/site-packages/mlflow/tracking/client.py", line 1195, in log_artifacts
    self._tracking_client.log_artifacts(run_id, local_dir, artifact_path)
  File "/home/airflow/.local/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 538, in log_artifacts
    self._get_artifact_repo(run_id).log_artifacts(local_dir, artifact_path)
  File "/home/airflow/.local/lib/python3.10/site-packages/mlflow/store/artifact/s3_artifact_repo.py", line 189, in log_artifacts
    self._upload_file(
  File "/home/airflow/.local/lib/python3.10/site-packages/mlflow/store/artifact/s3_artifact_repo.py", line 165, in _upload_file
    s3_client.upload_file(Filename=local_file, Bucket=bucket, Key=key, ExtraArgs=extra_args)
  File "/home/airflow/.local/lib/python3.10/site-packages/boto3/s3/inject.py", line 145, in upload_file
    return transfer.upload_file(
  File "/home/airflow/.local/lib/python3.10/site-packages/boto3/s3/transfer.py", line 377, in upload_file
    raise S3UploadFailedError(
boto3.exceptions.S3UploadFailedError: Failed to upload /tmp/tmpvo83hjgm/model/input_example.json to mlflows3/1/b7bc276d58a6471dba5980179852774d/artifacts/model/input_example.json: An error occurred (NoSuchBucket) when calling the PutObject operation: The specified bucket does not exist
[2025-11-01T22:46:51.069+0000] {taskinstance.py:1400} INFO - Marking task as UP_FOR_RETRY. dag_id=diabetes_training_pipeline, task_id=train_models, execution_date=20251001T000000, start_date=20251101T224642, end_date=20251101T224651
[2025-11-01T22:46:51.082+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 6 for task train_models (Failed to upload /tmp/tmpvo83hjgm/model/input_example.json to mlflows3/1/b7bc276d58a6471dba5980179852774d/artifacts/model/input_example.json: An error occurred (NoSuchBucket) when calling the PutObject operation: The specified bucket does not exist; 4263)
[2025-11-01T22:46:51.124+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2025-11-01T22:46:51.160+0000] {taskinstance.py:2778} INFO - 0 downstream tasks scheduled from follow-on schedule check
